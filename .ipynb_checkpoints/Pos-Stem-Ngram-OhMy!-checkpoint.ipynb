{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pos, Stem, and Ngrams: Oh My!\n",
    "<img src=\"images/churchill2.jpg\" height=\"200\" width=\"200\" align=\"left\">\n",
    "<center>\n",
    "<h3>In This Worksheet</h3> We will parse a new text using the methods described up to this point and introduce some new ways to characterize words and sentences in texts.\n",
    "<h3>The Data</h3> <strong>Speech, Blood Toil Tears and Sweat</strong><br><i>Sir Winston Churchill, May 13th 1940</i><br>\n",
    "This is Churchill's first speech as the prime minister of Great Britain.<br>\n",
    "https://www.youtube.com/watch?v=8TlkN-dcDCk\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's start by doing some imports and then loading up the Churchill data set we created in the last portion, using pandas's read_csv method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>is_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>on</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>friday</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>evening</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>last</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sent_id    token is_stop is_punct\n",
       "0        0       on    True    False\n",
       "1        0   friday   False    False\n",
       "2        0  evening   False    False\n",
       "3        0     last   False    False\n",
       "4        0        i    True    False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = 'data/parsed_churchill_blood.csv'\n",
    "speech = pd.read_csv(fp, index_col=0)\n",
    "speech.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngrams\n",
    "We will cover ngrams first because they are the most easy to visualize with our existing data.  Ngrams represent words that occur sequentially together.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('there', 'is')\n",
      "('is', 'a')\n",
      "('a', 'dog')\n",
      "('dog', 'in')\n",
      "('in', 'my')\n",
      "('my', 'purse')\n"
     ]
    }
   ],
   "source": [
    "ex_tokens = ['there', 'is', 'a', 'dog', 'in', 'my', 'purse']\n",
    "for ngram in nltk.ngrams(ex_tokens, 2):\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen ngram-related concepts at work previously when we looked at context and collocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make a text out of list of tokens from DataFrame\n",
    "text = nltk.Text(speech.token.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember when we generated the context of a word, we got back a FreqDist of all of the word pairs it was surrounded by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('with', 'to'): 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts = nltk.ContextIndex(text.tokens)\n",
    "contexts._word_to_contexts['germany']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this manually as well using ngrams.  Let's look for the word 'japanese' and count up all of its contexts as they occur in trigrams in which it is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('with', 'to'): 1})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "context_dict = Counter()\n",
    "for gram3 in nltk.ngrams(text.tokens, 3):\n",
    "    if gram3[1] == 'germany':\n",
    "        context_dict.update([(gram3[0],gram3[2])])\n",
    "context_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocations looked for pairs of words that were commonly seen occurring in the same window.  NLTK's BigramCollocationFinder actually looks at all ngrams in the text, so we will only consider the 4grams as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "british empire; wage war\n"
     ]
    }
   ],
   "source": [
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('for', 'the', 'british', 'empire'), ('the', 'british', 'empire', ','), ('british', 'empire', ',', 'no'), ('that', 'the', 'british', 'empire'), ('the', 'british', 'empire', 'has'), ('british', 'empire', 'has', 'stood')]\n"
     ]
    }
   ],
   "source": [
    "manual_coll_list = [('british', 'empire'), ('wage', 'war')]\n",
    "coll_dict = {}\n",
    "for gram4 in nltk.ngrams(text.tokens, 4):\n",
    "    for coll in manual_coll_list:\n",
    "        term1,term2 = coll\n",
    "        if term1 in gram4 and term2 in gram4:\n",
    "            coll_dict[coll] = coll_dict.get(coll,[]) + [gram4]\n",
    "            \n",
    "print(coll_dict[('british','empire')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Stemming is a crude way of shortening a word so that various lemmas of a word do not prevent us from identifying words as similar.\n",
    "\n",
    "For example, two words appear in our speech, 'attacked' and 'attack'.  In most cases, we want these to be understood as the same token, 'attack'.  NLTK's PorterStemmer can help us do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack\n",
      "attack\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.PorterStemmer()\n",
    "print(stemmer.stem('attacked'))\n",
    "print(stemmer.stem('attack'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see that in certain cases, though, the stemmer will fail to do what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n",
      "are\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem('is'))\n",
    "print(stemmer.stem('are'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing\n",
    "NLTK's default lemmatizer is the WordNet lemmatizer.  This looks at WordNet's morphy feature in order to generate a lexeme for the word, given the word's part of speech (default is noun).  The availabel part of speeches are:\n",
    "\n",
    "|POS|Representation|\n",
    "|---|---|\n",
    "|ADJ|'a'|\n",
    "|ADJ_SAT|'s'|\n",
    "|ADV|'r'|\n",
    "|NOUN|'n'|\n",
    "|VERB|'v'|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "lemmater = nltk.WordNetLemmatizer()\n",
    "print(lemmater.lemmatize('is', pos='v'))\n",
    "print(lemmater.lemmatize('are', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few more examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "half\n",
      "focus\n",
      "polarize\n",
      "wander\n"
     ]
    }
   ],
   "source": [
    "print(lemmater.lemmatize('halves'))\n",
    "print(lemmater.lemmatize('foci'))\n",
    "print(lemmater.lemmatize('polarizing', pos='v'))\n",
    "print(lemmater.lemmatize('wandering', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lemmatizer works pretty poorly on certain words, though, and if WordNet does not find a word, it will return the word unchanged, which is less useful than our stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merrily\n",
      "additional\n"
     ]
    }
   ],
   "source": [
    "print(lemmater.lemmatize('merrily', pos='r'))\n",
    "print(lemmater.lemmatize('additional', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way around this is to use the WordNet synset, which is the associated set of 'cognitive synonyms' for the word. \n",
    "\n",
    "Here we use 'merrily.r.1' to specify grabbing they synset for 'merrily' of type 'r' at index '0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('happily.r.01')\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "synset = wn.synset('merrily.r.0')\n",
    "print(synset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then grab all of its lemmas associated with the synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('happily.r.01.happily'), Lemma('happily.r.01.merrily'), Lemma('happily.r.01.mirthfully'), Lemma('happily.r.01.gayly'), Lemma('happily.r.01.blithely'), Lemma('happily.r.01.jubilantly')]\n"
     ]
    }
   ],
   "source": [
    "lemmas = synset.lemmas()\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once one gets to a lemma, you gain access to a whole new set of attributes, to include antonyms, homonyms, pertainyms, etc.  The point here is that the WordNet module is extremely powerful, but it is also very complex and standardizing a way to interact with it, all to acquire the best lexeme for your word, may not be the simplest concept.  This is why you may just want to use a stemmer, which is what we will choose to do in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speech['stem'] = speech.token.apply(stemmer.stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "We had to identify the POS in order to properly lemmatize certain words using the WordNet Lemmatizer.  But how do we get parts of speech in an automated fashion?  One option is to write your own model.  But NLTK also offers a pre-trained built in POS tagger.  We have already discussed some of the features that these models take into account, so let's see how the POS tagger works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'NNS'), ('say', 'VBP'), ('to', 'TO'), ('the', 'DT'), ('house', 'NN'), ('as', 'IN'), ('i', 'NN'), ('said', 'VBD'), ('to', 'TO'), ('ministers', 'NNS'), ('who', 'WP'), ('have', 'VBP'), ('joined', 'VBN'), ('this', 'DT'), ('government', 'NN'), (',', ','), ('i', 'NN'), ('have', 'VBP'), ('nothing', 'NN'), ('to', 'TO'), ('offer', 'VB'), ('but', 'CC'), ('blood', 'NN'), (',', ','), ('toil', 'NN'), (',', ','), ('tears', 'NNS'), (',', ','), ('and', 'CC'), ('sweat', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "sample_tokens = speech[speech.sent_id==20].token.tolist()\n",
    "print(pos_tag(sample_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|i|NNS||\n",
      "|say|VBP||\n",
      "|to|TO||\n",
      "|the|DT||\n",
      "|house|NN||\n",
      "|as|IN||\n",
      "|i|NN||\n",
      "|said|VBD||\n",
      "|to|TO||\n",
      "|ministers|NNS||\n",
      "|who|WP||\n",
      "|have|VBP||\n",
      "|joined|VBN||\n",
      "|this|DT||\n",
      "|government|NN||\n",
      "|,|,||\n",
      "|i|NN||\n",
      "|have|VBP||\n",
      "|nothing|NN||\n",
      "|to|TO||\n",
      "|offer|VB||\n",
      "|but|CC||\n",
      "|blood|NN||\n",
      "|,|,||\n",
      "|toil|NN||\n",
      "|,|,||\n",
      "|tears|NNS||\n",
      "|,|,||\n",
      "|and|CC||\n",
      "|sweat|NN||\n",
      "|.|.||\n"
     ]
    }
   ],
   "source": [
    "for token,tag in pos_tag(sample_tokens):\n",
    "    print('|'+token+'|'+tag+'||')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do all of these tags mean?\n",
    "Varies on tagset, but in general:\n",
    "\n",
    "|Tag|Description|Example|\n",
    "|---|---|---|\n",
    "|CC|conjunction, coordinating|and, or, but|\n",
    "|CD|cardinal number|five, three, 13%|\n",
    "|DT|determiner|the, a, these |\n",
    "|EX|existential there|there were six boys |\n",
    "|FW|foreign word|mais |\n",
    "|IN|conjunction, subordinating or preposition|of, on, before, unless |\n",
    "|JJ|adjective|nice, easy|\n",
    "|JJR|adjective, comparative|nicer, easier|\n",
    "|JJS|adjective, superlative|nicest, easiest |\n",
    "|LS|list item marker| |\n",
    "|MD|verb, modal auxillary|may, should |\n",
    "|NN|noun, singular or mass|tiger, chair, laughter |\n",
    "|NNS|noun, plural|tigers, chairs, insects |\n",
    "|NNP|noun, proper singular|Germany, God, Alice |\n",
    "|NNPS|noun, proper plural|we met two Christmases ago |\n",
    "|PDT|predeterminer|both his children |\n",
    "|POS|possessive ending|'s|\n",
    "|PRP|pronoun, personal|me, you, it |\n",
    "|PRP\\$|pronoun, possessive|my, your, our |\n",
    "|RB|adverb|extremely, loudly, hard  |\n",
    "|RBR|adverb, comparative|better |\n",
    "|RBS|adverb, superlative|best |\n",
    "|RP|adverb, particle|about, off, up |\n",
    "|SYM|symbol|None|\n",
    "|TO|infinitival to|what to do? |\n",
    "|UH|interjection|oh, oops, gosh |\n",
    "|VB|verb, base form|think |\n",
    "|VBZ|verb, 3rd person singular present|she thinks |\n",
    "|VBP|verb, non-3rd person singular present|I think |\n",
    "|VBD|verb, past tense|they thought |\n",
    "|VBN|verb, past participle|a sunken ship |\n",
    "|VBG|verb, gerund or present participle|thinking is fun |\n",
    "|WDT|wh-determiner|which, whatever, whichever |\n",
    "|WP|wh-pronoun, personal|what, who, whom |\n",
    "|WP\\$|wh-pronoun, possessive|whose, whosever |\n",
    "|WRB|wh-adverb|where, when |\n",
    "|.|punctuation mark, sentence closer|.;?* |\n",
    "|,|punctuation mark, comma|, |\n",
    "|:|punctuation mark, colon|: |\n",
    "|(|contextual separator, left paren|( |\n",
    "|)|contextual separator, right paren|) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can actually see that our POS's are..\n",
    "\n",
    "|Token|POS|Interpretation|\n",
    "|--|--|--|\n",
    "|i|NNS|noun, plural|\n",
    "|say|VBP|verb, non-3rd person singular present|\n",
    "|to|TO|\tinfinitival to|\n",
    "|the|DT|determiner|\n",
    "|house|NN|noun, singular or mass|\n",
    "|as|IN|conjunction, subordinating or preposition|\n",
    "|i|NN|noun, singular or mass|\n",
    "|said|VBD|verb, past tense|\n",
    "|to|TO|\tinfinitival to|\n",
    "|ministers|NNS|noun, plural|\n",
    "|who|WP|wh-pronoun, personal|\n",
    "|have|VBP|verb, non-3rd person singular present|\n",
    "|joined|VBN|verb, past participle|\n",
    "|this|DT|determiner|\n",
    "|government|NN|noun, singular or mass|\n",
    "\n",
    "Notice that \"i\" receives two different parts of speech at two different places.  This indicates that the context of the word is being considered when parts of speech are being determined (syntactic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the part of speech for a single token by writing a little function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_pos(token):\n",
    "    return pos_tag([token])[0][1]\n",
    "\n",
    "get_pos('i')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, we are much better off doing multiple tokens at once so that more context is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = speech.token.tolist()\n",
    "pos_tags = pos_tag(tokens)\n",
    "just_tags = [x[1] for x in pos_tags]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can create a new feature column for our tokens called 'pos' that stores the POS for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>is_punct</th>\n",
       "      <th>pos</th>\n",
       "      <th>generic_pos</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>on</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>IN</td>\n",
       "      <td>Misc</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>friday</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Adjective</td>\n",
       "      <td>friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>evening</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>VBG</td>\n",
       "      <td>Verb</td>\n",
       "      <td>even</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>last</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Adjective</td>\n",
       "      <td>last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Adjective</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sent_id    token is_stop is_punct  pos generic_pos    stem\n",
       "0        0       on    True    False   IN        Misc      on\n",
       "1        0   friday   False    False   JJ   Adjective  friday\n",
       "2        0  evening   False    False  VBG        Verb    even\n",
       "3        0     last   False    False   JJ   Adjective    last\n",
       "4        0        i    True    False   JJ   Adjective       i"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech['pos'] = pd.Series( just_tags )\n",
    "speech.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get an idea of the distributions of our parts of speech by using value_counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN      136\n",
       "IN       92\n",
       "DT       82\n",
       "JJ       43\n",
       "VB       40\n",
       ".        36\n",
       ",        28\n",
       "NNS      28\n",
       "CC       24\n",
       "VBP      23\n",
       "VBN      23\n",
       "TO       21\n",
       "PRP      19\n",
       "MD       17\n",
       "VBZ      16\n",
       "RB       14\n",
       "PRP$     13\n",
       "CD        6\n",
       "VBD       6\n",
       "VBG       5\n",
       "WP        4\n",
       ":         3\n",
       "''        3\n",
       "JJS       3\n",
       "POS       2\n",
       "RBS       2\n",
       "WDT       2\n",
       "JJR       2\n",
       "``        1\n",
       "RP        1\n",
       "PDT       1\n",
       "WRB       1\n",
       "EX        1\n",
       "Name: pos, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech.pos.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I look at just the noun-type parts of speech, I can find out the key focuses of the speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "i                 12\n",
       "house              6\n",
       "war                5\n",
       "victory            4\n",
       "survival           4\n",
       "nation             3\n",
       "today              3\n",
       "government         3\n",
       "administration     3\n",
       "task               3\n",
       "Name: token, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_pos = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "speech[speech.pos.isin(noun_pos)].token.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these generic groups of parts of speech are consistent regardless of text, let's make a dictionary to map each specific pos to a generic one based on the set: {'Pronoun', 'Adverb', 'Foreign', 'Determiner', 'Existential', 'Misc', 'Verb', 'Adjective', 'Noun', '.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pronoun', 'Adverb', 'Foreign', 'Determiner', 'Existential', 'Misc', 'Verb', 'Adjective', 'Noun', '.'}\n"
     ]
    }
   ],
   "source": [
    "pos_map = {\n",
    "    'CC':'Misc',\n",
    "    'CD':'Adjective',\n",
    "    'DT':'Determiner',\n",
    "    'EX':'Existential',\n",
    "    'FW':'Foreign',\n",
    "    'IN':'Misc',\n",
    "    'JJ':'Adjective',\n",
    "    'JJR':'Adjective',\n",
    "    'JJS':'Adjective',\n",
    "    'MD':'Verb',\n",
    "    'NN':'Noun',\n",
    "    'NNS':'Noun',\n",
    "    'NNP':'Noun',\n",
    "    'NNPS':'Noun',\n",
    "    'PDT':'Determiner',\n",
    "    'POS':'Misc',\n",
    "    'PRP':'Pronoun',\n",
    "    'PRP$':'Pronoun',\n",
    "    'RB':'Adverb',\n",
    "    'RBR':'Adverb',\n",
    "    'RBS':'Adverb',\n",
    "    'RP':'Adverb',\n",
    "    'SYM':'Misc',\n",
    "    'TO':'Misc',\n",
    "    'UH':'Misc',\n",
    "    'VB':'Verb',\n",
    "    'VBZ':'Verb',\n",
    "    'VBP':'Verb',\n",
    "    'VBD':'Verb',\n",
    "    'VBN':'Verb',\n",
    "    'VBG':'Verb',\n",
    "    'WDT':'Determiner',\n",
    "    'WP':'Pronoun',\n",
    "    'WP$':'Pronoun',\n",
    "    'WRB':'Adverb',\n",
    "    '.':'.',\n",
    "    ',':'.',\n",
    "    ':':'.',\n",
    "    '(':'Misc',\n",
    "    ')':'Misc',\n",
    "    \"''\":'Misc',\n",
    "    \"``\":'Misc',\n",
    "    '$':'Misc',\n",
    "}\n",
    "\n",
    "def map_generic_pos(tag):\n",
    "    return pos_map[tag]\n",
    "\n",
    "print(set(pos_map.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speech['generic_pos'] = speech.pos.apply(map_generic_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can repeat what was done previously, without having to manually enter the grouping we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "i                 12\n",
       "house              6\n",
       "war                5\n",
       "victory            4\n",
       "survival           4\n",
       "nation             3\n",
       "today              3\n",
       "government         3\n",
       "administration     3\n",
       "task               3\n",
       "ministers          3\n",
       "part               2\n",
       "resolution         2\n",
       "colleagues         2\n",
       "empire             2\n",
       "parliament         2\n",
       "air                2\n",
       "strength           2\n",
       "wage               2\n",
       "appointment        2\n",
       "Name: token, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech[speech.generic_pos=='Noun'].token.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "many         4\n",
       "one          3\n",
       "other        3\n",
       "i            2\n",
       "united       2\n",
       "new          2\n",
       "necessary    2\n",
       "british      2\n",
       "grievous     1\n",
       "monstrous    1\n",
       "Name: token, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech[speech.generic_pos=='Adjective'].token.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if our answers change when we look at the stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mani         4\n",
       "other        3\n",
       "one          3\n",
       "british      2\n",
       "new          2\n",
       "i            2\n",
       "necessari    2\n",
       "unit         2\n",
       "victori      2\n",
       "1            1\n",
       "Name: stem, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech[speech.generic_pos=='Adjective'].stem.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can get an idea of how often the speech uses each generic pos type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Noun           0.234957\n",
       "Misc           0.204871\n",
       "Verb           0.186246\n",
       "Determiner     0.121777\n",
       ".              0.095989\n",
       "Adjective      0.077364\n",
       "Pronoun        0.051576\n",
       "Adverb         0.025788\n",
       "Existential    0.001433\n",
       "Name: generic_pos, dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech.generic_pos.value_counts()/len(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
