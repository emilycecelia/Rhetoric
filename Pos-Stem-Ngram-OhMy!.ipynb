{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pos, Stem, and Ngrams: Oh My!\n",
    "<img src=\"images/fdr.jpg\" height=\"200\" width=\"200\" align=\"left\">\n",
    "<center>\n",
    "<h3>In This Worksheet</h3> We will parse a new text using the methods described up to this point and introduce some new ways to characterize words and sentences in texts.\n",
    "<h3>The Data</h3> <strong>Pearl Harbor Address to the Nation</strong><br><i>President Franklin Delano Roosevelt, December 8th, 1941</i><br>\n",
    "This is FDR's call to war to the American people after Pearl Harbor.<br>\n",
    "https://youtu.be/3VqQAf74fsE\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_punct</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>mr.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>vice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>president</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>mr.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  is_punct is_stop  sent_id      token\n",
       "0    False   False        1        mr.\n",
       "1    False   False        1       vice\n",
       "2    False   False        1  president\n",
       "3     True   False        1          ,\n",
       "4    False   False        1        mr."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = 'data/FDR-PearlHarbor_parsed.csv'\n",
    "speech = pd.read_csv(fp, index_col=0)\n",
    "speech.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'NN'), ('saw', 'VBD'), ('a', 'DT'), ('dog', 'NN'), ('running', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "tokenized_sentence = word_tokenize('I saw a dog running.'.lower())\n",
    "print(pos_tag(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Tag|Description|Example|\n",
    "|---|---|---|\n",
    "|CC|conjunction, coordinating|and, or, but|\n",
    "|CD|cardinal number|five, three, 13%|\n",
    "|DT|determiner|the, a, these |\n",
    "|EX|existential there|there were six boys |\n",
    "|FW|foreign word|mais |\n",
    "|IN|conjunction, subordinating or preposition|of, on, before, unless |\n",
    "|JJ|adjective|nice, easy|\n",
    "|JJR|adjective, comparative|nicer, easier|\n",
    "|JJS|adjective, superlative|nicest, easiest |\n",
    "|LS|list item marker| |\n",
    "|MD|verb, modal auxillary|may, should |\n",
    "|NN|noun, singular or mass|tiger, chair, laughter |\n",
    "|NNS|noun, plural|tigers, chairs, insects |\n",
    "|NNP|noun, proper singular|Germany, God, Alice |\n",
    "|NNPS|noun, proper plural|we met two Christmases ago |\n",
    "|PDT|predeterminer|both his children |\n",
    "|POS|possessive ending|'s|\n",
    "|PRP|pronoun, personal|me, you, it |\n",
    "|PRP\\$|pronoun, possessive|my, your, our |\n",
    "|RB|adverb|extremely, loudly, hard  |\n",
    "|RBR|adverb, comparative|better |\n",
    "|RBS|adverb, superlative|best |\n",
    "|RP|adverb, particle|about, off, up |\n",
    "|SYM|symbol|None|\n",
    "|TO|infinitival to|what to do? |\n",
    "|UH|interjection|oh, oops, gosh |\n",
    "|VB|verb, base form|think |\n",
    "|VBZ|verb, 3rd person singular present|she thinks |\n",
    "|VBP|verb, non-3rd person singular present|I think |\n",
    "|VBD|verb, past tense|they thought |\n",
    "|VBN|verb, past participle|a sunken ship |\n",
    "|VBG|verb, gerund or present participle|thinking is fun |\n",
    "|WDT|wh-determiner|which, whatever, whichever |\n",
    "|WP|wh-pronoun, personal|what, who, whom |\n",
    "|WP\\$|wh-pronoun, possessive|whose, whosever |\n",
    "|WRB|wh-adverb|where, when |\n",
    "|.|punctuation mark, sentence closer|.;?* |\n",
    "|,|punctuation mark, comma|, |\n",
    "|:|punctuation mark, colon|: |\n",
    "|(|contextual separator, left paren|( |\n",
    "|)|contextual separator, right paren|) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pos(word):\n",
    "    return pos_tag([word])[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parsed_speech' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-468138eae78d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#much faster when given as list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpos_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_speech\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mjust_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tags\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mparsed_speech\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mjust_tags\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parsed_speech' is not defined"
     ]
    }
   ],
   "source": [
    "#much faster when given as list\n",
    "pos_tags = pos_tag(parsed_speech.token.tolist())\n",
    "just_tags = [x[1] for x in pos_tags]\n",
    "parsed_speech['pos'] = pd.Series( just_tags )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parsed_speech' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3529f6f60a59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparsed_speech\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'parsed_speech' is not defined"
     ]
    }
   ],
   "source": [
    "parsed_speech.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parsed_speech' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f3f163031db1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparsed_speech\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'parsed_speech' is not defined"
     ]
    }
   ],
   "source": [
    "parsed_speech.pos.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parsed_speech' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-106679abe270>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnoun_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'NN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NNS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NNP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NNPS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparsed_speech\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparsed_speech\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoun_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'parsed_speech' is not defined"
     ]
    }
   ],
   "source": [
    "noun_pos = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "parsed_speech[parsed_speech.pos.isin(noun_pos)].token.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parsed_speech' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-26608a63ff0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mparsed_speech\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stem'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_speech\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'parsed_speech' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "parsed_speech['stem'] = parsed_speech.token.apply(stemmer.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>is_punct</th>\n",
       "      <th>pos</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>mr.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>JJ</td>\n",
       "      <td>mr.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>vice</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NN</td>\n",
       "      <td>vice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>president</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NN</td>\n",
       "      <td>presid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>,</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>mr.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NN</td>\n",
       "      <td>mr.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sent_id      token is_stop is_punct pos    stem\n",
       "0        0        mr.   False    False  JJ     mr.\n",
       "1        0       vice   False    False  NN    vice\n",
       "2        0  president   False    False  NN  presid\n",
       "3        0          ,   False     True   ,       ,\n",
       "4        0        mr.   False    False  NN     mr."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_speech.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aardwolf'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('aardwolves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_stops = parsed_speech[ (parsed_speech['is_stop'] == False) \n",
    "                         & (parsed_speech.is_punct == False) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "japanese     10\n",
       "american      6\n",
       "attacked      6\n",
       "forces        6\n",
       "united        6\n",
       "states        6\n",
       "people        5\n",
       "japan         5\n",
       "attack        5\n",
       "yesterday     4\n",
       "Name: token, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " no_stops.token.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attack      11\n",
       "japanes     10\n",
       "state        9\n",
       "unit         6\n",
       "forc         6\n",
       "american     6\n",
       "japan        5\n",
       "island       5\n",
       "peopl        5\n",
       "last         4\n",
       "Name: stem, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stops.stem.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CONVERT EVERYTHING TO SENTENCE thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mr.', 'vice'), ('vice', 'president'), ('president', ','), (',', 'mr.'), ('mr.', 'speaker'), ('speaker', ','), (',', 'members'), ('members', 'of'), ('of', 'the'), ('the', 'senate'), ('senate', ','), (',', 'and'), ('and', 'of'), ('of', 'the'), ('the', 'house'), ('house', 'of'), ('of', 'representatives'), ('representatives', ':'), (':', 'yesterday'), ('yesterday', ','), (',', 'december'), ('december', '7th'), ('7th', ','), (',', '1941'), ('1941', '--'), ('--', 'a'), ('a', 'date'), ('date', 'which'), ('which', 'will'), ('will', 'live'), ('live', 'in'), ('in', 'infamy'), ('infamy', '--'), ('--', 'the'), ('the', 'united'), ('united', 'states'), ('states', 'of'), ('of', 'america'), ('america', 'was'), ('was', 'suddenly'), ('suddenly', 'and'), ('and', 'deliberately'), ('deliberately', 'attacked'), ('attacked', 'by'), ('by', 'naval'), ('naval', 'and'), ('and', 'air'), ('air', 'forces'), ('forces', 'of'), ('of', 'the'), ('the', 'empire'), ('empire', 'of'), ('of', 'japan'), ('japan', '.')]\n"
     ]
    }
   ],
   "source": [
    "#ngrams on sentences\n",
    "from nltk import ngrams\n",
    "first_tokens = parsed_speech[parsed_speech.sent_id==0].token.tolist()\n",
    "bigrams = ngrams(first_tokens, 2)\n",
    "print(list(bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mr.', 'vice', 'president'), ('vice', 'president', ','), ('president', ',', 'mr.'), (',', 'mr.', 'speaker'), ('mr.', 'speaker', ','), ('speaker', ',', 'members'), (',', 'members', 'of'), ('members', 'of', 'the'), ('of', 'the', 'senate'), ('the', 'senate', ','), ('senate', ',', 'and'), (',', 'and', 'of'), ('and', 'of', 'the'), ('of', 'the', 'house'), ('the', 'house', 'of'), ('house', 'of', 'representatives'), ('of', 'representatives', ':'), ('representatives', ':', 'yesterday'), (':', 'yesterday', ','), ('yesterday', ',', 'december'), (',', 'december', '7th'), ('december', '7th', ','), ('7th', ',', '1941'), (',', '1941', '--'), ('1941', '--', 'a'), ('--', 'a', 'date'), ('a', 'date', 'which'), ('date', 'which', 'will'), ('which', 'will', 'live'), ('will', 'live', 'in'), ('live', 'in', 'infamy'), ('in', 'infamy', '--'), ('infamy', '--', 'the'), ('--', 'the', 'united'), ('the', 'united', 'states'), ('united', 'states', 'of'), ('states', 'of', 'america'), ('of', 'america', 'was'), ('america', 'was', 'suddenly'), ('was', 'suddenly', 'and'), ('suddenly', 'and', 'deliberately'), ('and', 'deliberately', 'attacked'), ('deliberately', 'attacked', 'by'), ('attacked', 'by', 'naval'), ('by', 'naval', 'and'), ('naval', 'and', 'air'), ('and', 'air', 'forces'), ('air', 'forces', 'of'), ('forces', 'of', 'the'), ('of', 'the', 'empire'), ('the', 'empire', 'of'), ('empire', 'of', 'japan'), ('of', 'japan', '.')]\n"
     ]
    }
   ],
   "source": [
    "trigrams = ngrams(first_tokens, 3)\n",
    "print(list(trigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
