{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pos, Stem, and Ngrams: Oh My!\n",
    "<img src=\"images/fdr.jpg\" height=\"200\" width=\"200\" align=\"left\">\n",
    "<center>\n",
    "<h3>In This Worksheet</h3> We will parse a new text using the methods described up to this point and introduce some new ways to characterize words and sentences in texts.\n",
    "<h3>The Data</h3> <strong>Pearl Harbor Address to the Nation</strong><br><i>President Franklin Delano Roosevelt, December 8th, 1941</i><br>\n",
    "This is FDR's call to war to the American people after Pearl Harbor.<br>\n",
    "https://youtu.be/3VqQAf74fsE\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's start by doing some imports and then loading up the FDR data set we created in the last exercise, using pandas's read_csv method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_punct</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>mr.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>vice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>president</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>mr.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  is_punct is_stop  sent_id      token\n",
       "0    False   False        1        mr.\n",
       "1    False   False        1       vice\n",
       "2    False   False        1  president\n",
       "3     True   False        1          ,\n",
       "4    False   False        1        mr."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = 'data/FDR-PearlHarbor_parsed.csv'\n",
    "speech = pd.read_csv(fp, index_col=0)\n",
    "speech.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngrams\n",
    "We will cover ngrams first because they are the most easy to visualize with our existing data.  Ngrams represent words that occur sequentially together.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('there', 'is')\n",
      "('is', 'a')\n",
      "('a', 'dog')\n",
      "('dog', 'in')\n",
      "('in', 'my')\n",
      "('my', 'purse')\n"
     ]
    }
   ],
   "source": [
    "ex_tokens = ['there', 'is', 'a', 'dog', 'in', 'my', 'purse']\n",
    "for ngram in nltk.ngrams(ex_tokens, 2):\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen ngram-related concepts at work previously when we looked at context and collocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make a text out of list of tokens from DataFrame\n",
    "text = nltk.Text(speech.token.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember when we generated the context of a word, we got back a FreqDist of all of the word pairs it was surrounded by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({(',', 'forces'): 3,\n",
       "          ('after', 'air'): 1,\n",
       "          ('the', 'ambassador'): 1,\n",
       "          ('the', 'attacked'): 2,\n",
       "          ('the', 'empire'): 1,\n",
       "          ('the', 'government'): 2})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts = nltk.ContextIndex(text.tokens)\n",
    "contexts._word_to_contexts['japanese']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this manually as well using ngrams.  Let's look for the word 'japanese' and count up all of its contexts as they occur in trigrams in which it is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(',', 'forces'): 3,\n",
       "         ('after', 'air'): 1,\n",
       "         ('the', 'ambassador'): 1,\n",
       "         ('the', 'attacked'): 2,\n",
       "         ('the', 'empire'): 1,\n",
       "         ('the', 'government'): 2})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "context_dict = Counter()\n",
    "for gram3 in nltk.ngrams(text.tokens, 3):\n",
    "    if gram3[1] == 'japanese':\n",
    "        context_dict.update([(gram3[0],gram3[2])])\n",
    "context_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocations looked for pairs of words that were commonly seen occurring in the same window.  NLTK's BigramCollocationFinder actually looks at all ngrams in the text, so we will only consider the 4grams as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "united states; last night; december 7th; forces attacked; japanese\n",
      "forces; japanese government; japanese attacked\n"
     ]
    }
   ],
   "source": [
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 'japanese', 'forces', 'attacked'), ('japanese', 'forces', 'attacked', 'hong'), (',', 'japanese', 'forces', 'attacked'), ('japanese', 'forces', 'attacked', 'guam'), (',', 'japanese', 'forces', 'attacked'), ('japanese', 'forces', 'attacked', 'the'), (',', 'the', 'japanese', 'attacked'), ('the', 'japanese', 'attacked', 'wake'), ('japanese', 'attacked', 'wake', 'island'), (',', 'the', 'japanese', 'attacked'), ('the', 'japanese', 'attacked', 'midway'), ('japanese', 'attacked', 'midway', 'island')]\n"
     ]
    }
   ],
   "source": [
    "manual_coll_list = [('united', 'states'), ('last', 'night'), ('december', '7th'), ('forces', 'attacked'), \n",
    "                    ('japanese', 'forces'), ('japanese', 'government'), ('japanese', 'attacked')]\n",
    "coll_dict = {}\n",
    "for gram4 in nltk.ngrams(text.tokens, 4):\n",
    "    for coll in manual_coll_list:\n",
    "        term1,term2 = coll\n",
    "        if term1 in gram4 and term2 in gram4:\n",
    "            coll_dict[coll] = coll_dict.get(coll,[]) + [gram4]\n",
    "            \n",
    "print(coll_dict[('japanese','attacked')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Stemming is a crude way of shortening a word so that various lemmas of a word do not prevent us from identifying words as similar.\n",
    "\n",
    "For example, two words appear in our speech, 'attacked' and 'attack'.  In most cases, we want these to be understood as the same token, 'attack'.  NLTK's PorterStemmer can help us do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack\n",
      "attack\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.PorterStemmer()\n",
    "print(stemmer.stem('attacked'))\n",
    "print(stemmer.stem('attack'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see that in certain cases, though, the stemmer will fail to do what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n",
      "are\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem('is'))\n",
    "print(stemmer.stem('are'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing\n",
    "NLTK's default lemmatizer is the WordNet lemmatizer.  This looks at WordNet's morphy feature in order to generate a lexeme for the word, given the word's part of speech (default is noun).  The availabel part of speeches are:\n",
    "\n",
    "|POS|Representation|\n",
    "|---|---|\n",
    "|ADJ|'a'|\n",
    "|ADJ_SAT|'s'|\n",
    "|ADV|'r'|\n",
    "|NOUN|'n'|\n",
    "|VERB|'v'|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "lemmater = nltk.WordNetLemmatizer()\n",
    "print(lemmater.lemmatize('is', pos='v'))\n",
    "print(lemmater.lemmatize('are', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few more examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "half\n",
      "focus\n",
      "polarize\n",
      "wander\n"
     ]
    }
   ],
   "source": [
    "print(lemmater.lemmatize('halves'))\n",
    "print(lemmater.lemmatize('foci'))\n",
    "print(lemmater.lemmatize('polarizing', pos='v'))\n",
    "print(lemmater.lemmatize('wandering', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lemmatizer works pretty poorly on certain words, though, and if WordNet does not find a word, it will return the word unchanged, which is less useful than our stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merrily\n",
      "additional\n"
     ]
    }
   ],
   "source": [
    "print(lemmater.lemmatize('merrily', pos='r'))\n",
    "print(lemmater.lemmatize('additional', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way around this is to use the WordNet synset, which is the associated set of 'cognitive synonyms' for the word. \n",
    "\n",
    "Here we use 'merrily.r.1' to specify grabbing they synset for 'merrily' of type 'r' at index '0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('happily.r.01')\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "synset = wn.synset('merrily.r.0')\n",
    "print(synset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then grab all of its lemmas associated with the synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('happily.r.01.happily'), Lemma('happily.r.01.merrily'), Lemma('happily.r.01.mirthfully'), Lemma('happily.r.01.gayly'), Lemma('happily.r.01.blithely'), Lemma('happily.r.01.jubilantly')]\n"
     ]
    }
   ],
   "source": [
    "lemmas = synset.lemmas()\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once one gets to a lemma, you gain access to a whole new set of attributes, to include antonyms, homonyms, pertainyms, etc.  The point here is that the WordNet module is extremely powerful, but it is also very complex and standardizing a way to interact with it, all to acquire the best lexeme for your word, may not be the simplest concept.  This is why you may just want to use a stemmer.  But we will play with WordNet a little more later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "We had to identify the POS in order to properly lemmatize certain words using the WordNet Lemmatizer.  But how do we get parts of speech in an automated fashion?  One option is to write your own model.  But NLTK also offers a pre-trained built in POS tagger.  We have already discussed some of the features that these models take into account, so let's see how the POS tagger works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mr.', 'JJ'), ('vice', 'NN'), ('president', 'NN'), (',', ','), ('mr.', 'NN'), ('speaker', 'NN'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "sample_tokens = speech.token.tolist()[:10]\n",
    "print(pos_tag(sample_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do all of these tags mean?\n",
    "Varies on tagset, but in general:\n",
    "\n",
    "|Tag|Description|Example|\n",
    "|---|---|---|\n",
    "|CC|conjunction, coordinating|and, or, but|\n",
    "|CD|cardinal number|five, three, 13%|\n",
    "|DT|determiner|the, a, these |\n",
    "|EX|existential there|there were six boys |\n",
    "|FW|foreign word|mais |\n",
    "|IN|conjunction, subordinating or preposition|of, on, before, unless |\n",
    "|JJ|adjective|nice, easy|\n",
    "|JJR|adjective, comparative|nicer, easier|\n",
    "|JJS|adjective, superlative|nicest, easiest |\n",
    "|LS|list item marker| |\n",
    "|MD|verb, modal auxillary|may, should |\n",
    "|NN|noun, singular or mass|tiger, chair, laughter |\n",
    "|NNS|noun, plural|tigers, chairs, insects |\n",
    "|NNP|noun, proper singular|Germany, God, Alice |\n",
    "|NNPS|noun, proper plural|we met two Christmases ago |\n",
    "|PDT|predeterminer|both his children |\n",
    "|POS|possessive ending|'s|\n",
    "|PRP|pronoun, personal|me, you, it |\n",
    "|PRP\\$|pronoun, possessive|my, your, our |\n",
    "|RB|adverb|extremely, loudly, hard  |\n",
    "|RBR|adverb, comparative|better |\n",
    "|RBS|adverb, superlative|best |\n",
    "|RP|adverb, particle|about, off, up |\n",
    "|SYM|symbol|None|\n",
    "|TO|infinitival to|what to do? |\n",
    "|UH|interjection|oh, oops, gosh |\n",
    "|VB|verb, base form|think |\n",
    "|VBZ|verb, 3rd person singular present|she thinks |\n",
    "|VBP|verb, non-3rd person singular present|I think |\n",
    "|VBD|verb, past tense|they thought |\n",
    "|VBN|verb, past participle|a sunken ship |\n",
    "|VBG|verb, gerund or present participle|thinking is fun |\n",
    "|WDT|wh-determiner|which, whatever, whichever |\n",
    "|WP|wh-pronoun, personal|what, who, whom |\n",
    "|WP\\$|wh-pronoun, possessive|whose, whosever |\n",
    "|WRB|wh-adverb|where, when |\n",
    "|.|punctuation mark, sentence closer|.;?* |\n",
    "|,|punctuation mark, comma|, |\n",
    "|:|punctuation mark, colon|: |\n",
    "|(|contextual separator, left paren|( |\n",
    "|)|contextual separator, right paren|) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can actually see that our POS's are..\n",
    "\n",
    "|Token|POS|Interpretation|\n",
    "|--|--|--|\n",
    "|mr.|JJ|adjective|\n",
    "|vice|NN|noun singular|\n",
    "|president|NN|noun singular|\n",
    "|,|,|,|\n",
    "|mr.|NN|noun singular|\n",
    "|speaker|NN|noun singular|\n",
    "|,|,|,|\n",
    "|members|NNS|noun plural|\n",
    "|of|IN|conjunction or preposition|\n",
    "|the|DT|determiner|\n",
    "\n",
    "Notice that mr. receives two different parts of speech at two different places.  This indicates that the context of the word is being considered when parts of speech are being determined (syntactic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the part of speech for a single token by writing a little function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_pos(token):\n",
    "    return pos_tag([token])[0][1]\n",
    "\n",
    "get_pos('mr.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, we are much better off doing multiple tokens at once so that more context is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = speech.token.tolist()\n",
    "pos_tags = pos_tag(tokens)\n",
    "just_tags = [x[1] for x in pos_tags]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can create a new feature column for our tokens called 'pos' that stores the POS for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_punct</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>mr.</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>vice</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>president</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>mr.</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  is_punct is_stop  sent_id      token pos\n",
       "0    False   False        1        mr.  JJ\n",
       "1    False   False        1       vice  NN\n",
       "2    False   False        1  president  NN\n",
       "3     True   False        1          ,   ,\n",
       "4    False   False        1        mr.  NN"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech['pos'] = pd.Series( just_tags )\n",
    "speech.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN      111\n",
       "IN       71\n",
       "JJ       62\n",
       "DT       60\n",
       "NNS      36\n",
       ",        30\n",
       ".        26\n",
       "CC       24\n",
       "RB       22\n",
       "VB       20\n",
       "PRP      15\n",
       "VBN      14\n",
       "PRP$     14\n",
       "VBD      14\n",
       "VBP      13\n",
       "TO       11\n",
       "MD       11\n",
       "VBZ       6\n",
       "CD        5\n",
       ":         4\n",
       "VBG       3\n",
       "WDT       3\n",
       "WRB       2\n",
       "EX        1\n",
       "Name: pos, dtype: int64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech.pos.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forces        6\n",
       "states        6\n",
       "attack        5\n",
       "people        5\n",
       "japan         5\n",
       "yesterday     4\n",
       "night         4\n",
       "i             4\n",
       "government    3\n",
       "nation        3\n",
       "Name: token, dtype: int64"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_pos = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "speech[speech.pos.isin(noun_pos)].token.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
