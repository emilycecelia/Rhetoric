{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Rhetorical Devices Using NLTK\n",
    "Now that we have covered some rhetorical devices, let's review them and add code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#utilities\n",
    "def is_stopword(token):\n",
    "    stops = stopwords.words('english').copy()\n",
    "    return token in stops\n",
    "\n",
    "def is_punctuation(token):\n",
    "    return token in string.punctuation\n",
    "\n",
    "def is_vowel(char):\n",
    "    if char in ('a','e','i','o','u','y'):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rhetorical Question\n",
    "\n",
    "### What is it?\n",
    "* A question asked for some purpose other than to acquire information\n",
    "* “Why oh why me?”\n",
    "* “Who do you think you are?!”\n",
    "\n",
    "### How can we find it?\n",
    "* Our texts are monologues\n",
    "* Any question is not expected to be answered\n",
    "* Look for sentences ending in “?”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['now, what did they mean by that?', 'did they mean, my friends, to say that all me were created equal and that that meant that any one man was born to inherit $10,000,000,000 and that another child was to be born to inherit nothing?', \"did that mean, my friends, that someone would come into this world without having had an opportunity, of course, to have hit one lick of work, should be born with more than it and all of its children and children's children could ever dispose of, but that another one would have to be born into a life of starvation?\", 'that was not the meaning of the declaration of independence when it said that all men are created equal of \"that we hold that all men are created equal.\"']\n"
     ]
    }
   ],
   "source": [
    "#http://www.americanrhetoric.com/speeches/hueyplongking.htm\n",
    "huey = '''Now, what did they mean by that? Did they mean, my friends, to say that all me were created equal and that that meant that any one man was born to inherit $10,000,000,000 and that another child was to be born to inherit nothing?\n",
    "\n",
    "Did that mean, my friends, that someone would come into this world without having had an opportunity, of course, to have hit one lick of work, should be born with more than it and all of its children and children's children could ever dispose of, but that another one would have to be born into a life of starvation?\n",
    "\n",
    "That was not the meaning of the Declaration of Independence when it said that all men are created equal of \"That we hold that all men are created equal.\"'''\n",
    "\n",
    "huey_sents = nltk.sent_tokenize(huey.lower())\n",
    "print(huey_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def is_rhetorical(sent):\n",
    "    if sent.endswith('?'):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "for sent in huey_sents:\n",
    "    print(is_rhetorical(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epitrophe\n",
    "###  What is it?\n",
    "* Ending successive phrases in similar manner\n",
    "* “What lies behind us and what lies before us are tiny compared to what lies within us.\" —Emerson\n",
    "\n",
    "### How can we find it?\n",
    "* Just a single word repeating is not epitrophe\n",
    "* We need to look at the sentence structure\n",
    "* If surrounded by the same parts of speech, consider epitrophe\n",
    "* Should consider looking within a sentence and between successive sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('what', 'lies', 'behind'), ('what', 'lies', 'before')), (('what', 'lies', 'behind'), ('what', 'lies', 'within'))]\n"
     ]
    }
   ],
   "source": [
    "#Identify words,ngrams that occur multiple times\n",
    "#Get context of word\n",
    "#POS tag context\n",
    "from collections import Counter\n",
    "\n",
    "def has_epitrophe(sent):\n",
    "    epitrophe_instances = []\n",
    "    \n",
    "    tokens = nltk.word_tokenize(sent.lower())\n",
    "    word_dist = Counter(tokens)\n",
    "    repeated_words = [word for word,count in word_dist.items() if count > 1]\n",
    "    \n",
    "    trigrams = nltk.ngrams(tokens,3)\n",
    "    for word in repeated_words:\n",
    "        anchor = None\n",
    "        for trigram in trigrams:\n",
    "            if trigram[1] == word:\n",
    "                tags = nltk.pos_tag(trigram)\n",
    "                if anchor:\n",
    "                    if tags[0][1] == anchor[0][1]:\n",
    "                        epitrophe_instances.append((tuple([word for word,pos in anchor]), trigram))\n",
    "                else:\n",
    "                    anchor = tags\n",
    "        return epitrophe_instances\n",
    "        \n",
    "emerson = '''What lies behind us and what lies before us are tiny compared to what lies within us.'''\n",
    "emerson_sents = nltk.sent_tokenize(emerson.lower())\n",
    "\n",
    "for sent in emerson_sents:\n",
    "    print(has_epitrophe(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alliteration\n",
    "### What is it?\n",
    "The occurrence of the same letter or sound at the beginning of adjacent or closely connected words\n",
    "* \"They are part of the finest fighting force that the world has ever known. They have served tour after tour of duty in distant, different, and difficult places...\"  —Obama\n",
    "\n",
    "### How can we find it?\n",
    "* Look for successive words with similar sounds\n",
    "    * Same two first letters\n",
    "    * Same first vowel\n",
    "    * Same first consonant followed by a vowel\n",
    "* Remove stop words\n",
    "* Look for successive words and words one apart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"this generation of soldiers, sailors, airmen, marines, and coast guardsmen have volunteered in the time of certain danger.', 'they are part of the finest fighting force that the world has ever known.', 'they have served tour after tour of duty in distant, different, and difficult places...they are men and women -- white, black, and brown -- of all faiths and all stations -- all americans, serving together to protect our people, while giving others half a world away the chance to lead a better life....in today’s wars, there\\'s not always a simple ceremony that signals our troops’ success -- no surrender papers to be signed, or capital to be claimed....\"']\n"
     ]
    }
   ],
   "source": [
    "obama = '''\"This generation of soldiers, sailors, airmen, Marines, and Coast Guardsmen have volunteered in the time of certain danger. They are part of the finest fighting force that the world has ever known. They have served tour after tour of duty in distant, different, and difficult places...They are men and women -- white, black, and brown -- of all faiths and all stations -- all Americans, serving together to protect our people, while giving others half a world away the chance to lead a better life....In today’s wars, there's not always a simple ceremony that signals our troops’ success -- no surrender papers to be signed, or capital to be claimed....\"'''\n",
    "\n",
    "obama_sents = nltk.sent_tokenize(obama.lower())\n",
    "print(obama_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"this generation of soldiers, sailors, airmen, marines, and coast guardsmen have volunteered in the time of certain danger.\n",
      "[('soldiers', 'sailors')]\n",
      "1\n",
      "they are part of the finest fighting force that the world has ever known.\n",
      "[('finest', 'fighting'), ('fighting', 'force')]\n",
      "2\n",
      "they have served tour after tour of duty in distant, different, and difficult places...they are men and women -- white, black, and brown -- of all faiths and all stations -- all americans, serving together to protect our people, while giving others half a world away the chance to lead a better life....in today’s wars, there's not always a simple ceremony that signals our troops’ success -- no surrender papers to be signed, or capital to be claimed....\"\n",
      "[('tour', 'tour'), ('duty', 'distant'), ('distant', 'different'), ('different', 'difficult'), ('lead', 'better', 'life'), ('simple', 'ceremony', 'signals'), ('signals', 'troops’', 'success'), ('success', '--', 'surrender'), ('surrender', 'papers', 'signed')]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def has_alliteration(word1, word2):\n",
    "    gram00 = word1[0]\n",
    "    gram10 = word2[0]\n",
    "    \n",
    "    if gram00==gram10:\n",
    "        if is_vowel(gram00):\n",
    "            return True\n",
    "        else:\n",
    "            if len(word1) > 1 and len(word2) > 1:\n",
    "                gram01 = word1[1]\n",
    "                gram11 = word2[1]\n",
    "                if is_vowel(gram01) and is_vowel(gram11):\n",
    "                    return (word1, word2)\n",
    "                elif gram01 == gram11:\n",
    "                    return (word1, word2)\n",
    "    return None\n",
    "\n",
    "def count_alliteration(sent):\n",
    "    allit_instances = []\n",
    "    tokens = nltk.word_tokenize(sent.lower())\n",
    "    #ignore stopwords\n",
    "    tokens = [token for token in tokens if not(is_punctuation(token) or is_stopword(token))]\n",
    "    \n",
    "    bigrams = nltk.ngrams(tokens,2)\n",
    "    for one,two in bigrams:\n",
    "        if has_alliteration(one,two):\n",
    "            allit_instances.append((one,two))\n",
    "    trigrams = nltk.ngrams(tokens,3)\n",
    "    for one,two,three in trigrams:\n",
    "        #the not avoids double counting\n",
    "        if has_alliteration(one,three) and not has_alliteration(one,two):\n",
    "            allit_instances.append((one,two,three))\n",
    "    print(allit_instances)\n",
    "    return len(allit_instances)\n",
    "\n",
    "for sent in obama_sents:\n",
    "    print(sent)\n",
    "    print(count_alliteration(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epenalepsis\n",
    "### What is it?\n",
    "Repeating the same phrase at the beginning and end of a phrase \n",
    "“Believe not all you can hear, tell not all you believe.\" —Native American proverb\n",
    "### How can we find it?\n",
    "*You Try*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "native = '''Believe not all you can hear, tell not all you believe.'''\n",
    "\n",
    "def has_epenalepsis(sent):\n",
    "    \n",
    "    \n",
    "print(has_epanalepsis(native.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polyptoton\n",
    "### What is it?\n",
    "* Repeating a word in a sentence, but in a different form (think lemma vs lexeme)\n",
    "    * “The Greeks are strong, and skillful to their strength, fierce to their skill, and to their fierceness valiant” -- Shakespeare\n",
    "\n",
    "    \n",
    "### How can we find it?\n",
    "*You Try*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shake = '''The Greeks are strong, and skillful to their strength, fierce to their skill, and to their fierceness valiant.'''\n",
    "\n",
    "def has_polyptoton(sent):\n",
    "    \n",
    "    \n",
    "print(has_polyptoton(shake.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chiasmus\n",
    "### What is it?\n",
    "* Reordering parallel grammatical structures in a sentence/phrase\n",
    "    * \"Ask not what your country can do for you; ask what you can do for your country.\" John F. Kennedy \n",
    "\n",
    "### How can we find it?\n",
    "*You Try*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
